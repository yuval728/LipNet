# Server Configurations
model_store=/home/model-server/model-store
# Path to your model store
initial_workers=2
# Number of workers to start with per model
number_of_gpu=1
# Set the number of GPUs (set to 1 or more for GPU use)
job_queue_size=1000
# Size of the job queue for handling requests

# Worker Configurations
max_request_size=204857600
# Max size of a request (in bytes), e.g., ~200MB
default_response_timeout=120
# Max time (in seconds) to wait for a response
max_batch_delay=100
# Max time (in ms) to wait before batching requests
batch_size=4
# Batch size for inference
enable_envvars_config=true
# Allow environment variables to override config values

# Logging Configurations
log_level=INFO
# Set log level (DEBUG, INFO, WARN, ERROR)
ts_log_file=/home/model-server/logs/ts_log.log
# Path to the TorchServe log file
metrics_format=prometheus
# Format for metrics reporting
enable_metrics=true
# Enable/Disable metrics collection

# Metrics Reporting (Optional)
# metrics_interval=60                             
# Interval (in seconds) for metrics collection
# metrics_url=http://your-metrics-server:port     
# URL of the metrics server (Prometheus, etc.)

# Security Configurations
enable_ssl=false
# Enable HTTPS for serving requests
keystore_file=
# Path to the keystore file for SSL (if enabled)
keystore_password=
# Password for the keystore file

# Model Management
load_models=all
# Options: all, none, or model_name=model_version

# Thread Configuration
grpc_inference_thread_count=8
# Number of threads for gRPC inference
netty_client_threads=16
number_of_netty_threads=16
# Number of Netty threads (affects request concurrency)
